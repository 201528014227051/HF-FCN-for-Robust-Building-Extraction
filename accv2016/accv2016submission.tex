% Updated in February 2016 by Hwann-Tzong Chen
% Updated in May 2014 by Hideo Saito
% Updated in March 2012 by Yasuyuki Matsushita
% Updated in April 2002 by Antje Endemann, ...., and in March 2010 by Reinhard Klette
% Based on CVPR 07 and LNCS style, with modifications by DAF, AZ and elle 2008, AA 2010, ACCV 2010

\documentclass[runningheads]{llncs}
\usepackage{graphicx}
\usepackage{amsmath,amssymb} % define this before the line numbering.
\usepackage{ruler}
\usepackage{color}
\usepackage{subfigure}
\usepackage{mathrsfs}
%===========================================================
\begin{document}
\pagestyle{headings}
\mainmatter

\def\ACCV16SubNumber{***}  % Insert your submission number here

%===========================================================
\title{Building Rooftop Extraction in Aerial Image with Dual-Task Fully Convolutional Network} % Replace with your title
\titlerunning{ACCV-16 submission ID \ACCV16SubNumber}
\authorrunning{ACCV-16 submission ID \ACCV16SubNumber}

\author{Anonymous ACCV 2016 submission}
\institute{Paper ID \ACCV16SubNumber}

\maketitle
%===========================================================
\begin{abstract}
   Automatic building extraction from remote sensing image plays a critical role in  a diverse range of applications. However, it is  a huge challenge to extract any-size buildings with large variational appearances. In this article, we propose a novel multi-scale fully convolutional network(MSFCN), as the name suggests, which can handle multi-size objects only in a single convolutional neural network. Fully convolutional networks(FCNs) are proved to be a appropriate method to complete semantic segmentation task. It can take whole images as inputs without warping or cropping and output segmentation results directly.  However, it has less ability to  process a large number of small buildings in aerial image. For the sake of handling this problem, we integrate all predictions from layers with skipped receptive fields stage by stage. The experiment results test on a publicly available aerial imagery dataset proved that our proposed  methodology significantly  shorten time-consuming and surpass the performance of state-of-the-art.

\end{abstract}

%===========================================================
\section{Introduction}
   With the rapid development of remote sensing technologies and popularization of geospatial related commercial software, very high resolution satellite images are very easily accessible, these valuable data provides a huge fuel for interpreting real terrestrial scenes. Building rooftops is one the most important part of terrestrial objects because it is essential for a wide range of technologies, such as, urban planning, automated map making, 3D city modelling, disaster assessment, military reconnaissance, etc. However, it is  very costly and time-consuming for human experts to complete this task. Therefore, many researchers have made massive attempts to extract buildings automatically. 

% Though a series of approaches have been proposed to solve this problem, it is still a big challenge to  
%build a genetic and robust building extraction system. This is mainly due to the following two reasons. One is that a large amount of buildings are occluded by shadows stem from itself or other buildings, it causes a significant difficulty for image segmentation. The other is that building rooftops possess diverse shapes and colors, therefore, they can be easily confused with similar objects such as cars, roads, and courtyards. 

   In previous literatures, large amounts of efforts achieve good performance in extracting buildings with special color, shapes, textures, or surroundings. According to the information they concerned, these methods may fall into three classes. Though these techniques are different, the common point is there are a series of manually set rules or features in their systems.
   
   One popular way of extracting buildings is employing their shape information. It is observed that rooftops have more regular shapes, which usually are rectangular or combinations of several rectangles. A dozen years ago, Noronha and Nevatia \cite{noronha2001detection} designed a system that detects and constructs 3D models for rectilinear buildings from multiple aerial images. Firstly, hypotheses for rectangular rooftop were generated by grouping lines, then they were verified by searching for presence of predicted walls and shadows. Nosrati and Saeedi \cite{nosrati2009novel} pointed out that polygonal rooftops correspond to closed loops in a graph which represents the relationship between intersections of a pair of edge in an efficient way. In \cite{izadi2012three}, Izadi and Saeedi exploited a graph-based search to establish a set of rooftop hypotheses through examining the relationships of lines and line intersections. Cui \textit{et al.} \cite{cui2012complex} extracted buildings through the Hough transform (HT), but HT has notable drawbacks in parameter tuning and time complexity. Cote and Saeedi \cite{cote2013automatic} generated rooftop outline from selected corners in  multiple color and color-invariance spaces, further refined to fit the best possible boundaries through level-set curve evolution. In \cite{wang2015efficient}, Wang \textit{et al.}  presented a graph search-based perceptual grouping approach to hierarchically group line segments detected by EDLines \cite{akinlar2011edlines} into candidate rectangular buildings, computation complexity of the approach was reduced dramatically compared to \cite{noronha2001detection} \cite{izadi2012three} \cite{cote2013automatic} \cite{mayunga2007semi}. However, geometric primitives based methods suffer from three serious shortcomings. Firstly, they lack the ability of detecting arbitrarily shaped building rooftop. Secondly, they fail to  extract credible geometric features in buildings with inhomogeneous color distribution or low contrast with surroundings. Thirdly, it is hardly possible to process large-scale scenes because of their high computational-complexity.	
	
	Several studies reported that buildings are often composed of homogeneous regions with similar color or texture nearby shadows in remote sensing images. Spectral features is a distinctive feature for object detection, for instance, shadows are commonly dark grey or black, vegetations are usually green or yellow with particular textures, and main roads are dim gray with different road marks in most case. According to these prior knowledge mentioned above, Ghaffarian \textit{et al.}  \cite{ghaffarian2014automaticPFICA} proposed an purposive fast independent component analysis (PFastICA) technique to separate building area from remote sensing image. However, Ghaffarian's approach fails to detect the buildings with significantly different coloured rooftops. In \cite{ghaffarian2014automaticsupervised}, illumination direction and shadow area information of training samples were collected firstly, and then a improved parallelepiped classification method was applied to classify the image pixels into building and non-building areas. In \cite{chen2014shadow}, Chen \textit{et al.} proposed a supervised building detection framework. At first, source image was divided into super-pixels using the SLIC \cite{achanta2012slic} algorithm, then shadow patches are recognized using LDA color feature and the SVM classifier. The rough segmentation of buildings is employed by an adaptive regional growth algorithm that considers the spatial relationship between shadows and buildings. Finally, buildings are segmented accurately using a level set model. Dornaika \textit{et al.} \cite{dornaika2015object} proposed a similar framework,
Firstly, remote sensing image is segmentation  by statistical region merging (SRM) algorithm , hybrid descriptor composed by color histograms and local binary patterns is used to represent each segmented region. Finally, each region was classified using machine learning tools and a gallery of training descriptors. However, a major problem of these methods is that they have less capability to detect building rooftop with significantly varying illumination in different parts or no surrounding shadows.

   Other studies presented synthetic approaches with fusion of multiple features to extract building footprints from aerial images. Baluyan \textit{et al.} \cite{baluyan2013novel} proposed a method combining spectral and spatial features. Firstly, image is segmented into a set of rooftop candidates. Secondly, a SVM classifier is trained to distinguish rooftop regions or nonrooftop regions using extracted multiple features in dataset. Finally, "histogram method" is devised to detect missed rooftops in previous step. In \cite{ngoautomatic}, Ngo \textit{et al.} presented a novel approach for automated detection of rectangular buildings. At the first step, image is decomposed into small homogeneous regions as candidates. At the second step, a merging process is then performed over regions having similar spectral traits to produce rectilinear building region in accordance with position of shadows. Li \textit{et al.} \cite{li2015robust} proposed a higher order conditional random field (HCRF) based method, which incorporates both pixel-and segment-level information for the segmentation of rooftops. They claimed that the proposed model outperforms the best unsupervised methods at rooftops with complex structures and sizes. Nonetheless, feature selection and parameter tuning are considerable troubles.
    
\subsection{Related Works}
   In recent years, deep neural networks have been widely deployed in general image segmentation or scene labelling tasks. Mnih \cite{Mnih2013Machine} presented a patch-based framework for learning to label aerial images. A carefully designed neural network architecture is designed for predicting buildings in aerial imagery, and then the output of this network is processed by conditional random fields(CRFs). Satito \cite{Saito2016Multiple} improved Mnih's networks for extracting multiple kinds of objects simultaneously, two techniques consisting of model averaging with spatial displacement(MA) and channel-wise inhibited softmax (CIS) are introduced to enhance the  performance. However, these two methods need to crop test image into a fixed size, which not only  increase the time loss, but also break the integrity of building. For example, these method achieve bad performance for jumbo-size building (see Fig. \ref{fig:BadResults}). Meanwhile, it takes about 72s to solve a 1500 $\times$ 1500 image with model averaging in single computer. 
 
   Mapping buildings from aerial image is essentially a problem of semantic segmentation. Recent work suggests a number of methods in processing natural images.  Chen et al. \cite{chen14semantic} present a system which combine the responses at the final convulotional layer with a fully connected Conditional Random Field (CRF). The system is able to localize segment boundaries at a quite high level of accuracy. An end-to-end network  \cite{Zheng2015Conditional}  which integrates CRF modelling with CNNs avoids off-line post-processing methods for object delineation. Noh et al. \cite{Noh2015Learning} apply a deconvolution network to each proposal in an input image, and construct the final semantic segmentation map by combining the results from all proposals in a simple manner.
     
    Although these methods show promise in segmenting natural images, they have components not suited for building extraction. First, buildings are frequently occluded by shadows or trees (see Fig.\ref{fig:AerialImages} (a)). It is impossible to directly delineate building boundaries even for human experts. Nonetheless, human can estimate the actual footprints according to a strong prior knowledge that almost rooftops are regular polygons. Though \cite{chen14semantic} \cite{Zheng2015Conditional} achieve excellent performance in natural image, both of them can't achieve such a high intelligence.\textbf{ Second, buildings have significantly variational appearance even in a single one.} (see Fig.\ref{fig:AerialImages} (b)). Moreover, some buildings' appearance are very close to the plot on the ground or road. Based on our observation, there are a handful of samples emerged in PASACAL VOC 2012 dataset. Last but not the least, the number of objects in a remote sensing image is in a wide range. For example, some imagery  include a large number of tiny buildings Fig.\ref{fig:AerialImages} (c) and some ones are composed of moderate quantity of small-scale rooftops and a few of large-scale rooftops Fig.\ref{fig:AerialImages} (d). \cite{Noh2015Learning} claimed that it handles objects in multiple scales, but it only suitable to multi-class object segmentation. Therefore, a system which can handle any-size rooftops is in high demand. 
  
   The rest of this article is organized as follows. The section 2.1 outlines some key ideas of fully convolutional network(FCN), the section 2.2 provides details of our neural network architecture. In section 2.3 the formulation of our system is proposed. The section 4.1 presents the datasets which is used for training and testing in our experiments. The section 3.2  introduces training settings and strategies of our proposed network.  The section 3.3, we compare our results with two patch based methods using same dataset with us. In section 4, we discuss the experimental results and summarize whole article. 
	
%===========================================================
\section{Network Architecture} 
    In this section, we firstly introduce a well-known semantic segmentation network, called fully convolutional network(FCN)\cite{Long2014Fully}. We attempt to extract footprints of building  using  architecture proposed by author and two modified versions. Our experiments show that these networks are not enough to accomplish building   extraction task. \textbf{Therefore, we introduce a multi-scale fully convolutional network(MSFCN) for extracting rooftops, it turns out to be achieve state-of-the-art performance in a public aerial image dataset.}
         
\begin{figure}
\centering
\label{fig:AerialImages}
\includegraphics[width=120mm]{AerialImages}
\caption{Examples of aerial image}
\end{figure}
 
\subsection{Fully Convolutional Network}
    As mentioned in introduction, patch-based building extraction methods \cite{Mnih2013Machine} \cite{Saito2016Multiple} suffer from shortcoming in processing big buildings because of the fixed-size inputs. As far as we know, fully convolutional network(FCN) takes whole image as inputs and directly outputs building rooftops by one pass of forward propagation. Because it can process input images of any sizes without warping or cropping, integrality of object is protected much better. 
	
	Long  \textit{et al.} put forward the concept of fully convolutional network(FCN) in \cite{Long2014Fully} for the first time. \textbf{It said: ``While a general deep net computes a general nonlinear function, a net with only layers of this form computes a nonlinear filter, which we call a deep filter or fully convolutional network.''} In practice, the fully connected layers in traditional CNNs are transformed to convolutional layers with 1 $\times$ 1 kernels. Due to the mechanism of pooling, the output of the network is a coarse heat map. For pixelwise prediction, skip-net technique is used to connect various coarse outputs back to pixels.  For example, the framework of FCN-8s (see Fig. \ref{fig:Fig1FCN8s}) is described as following. We firstly obtain the 16 stride predictions by fusing the predictions of pooling4 with the 2 $\times$ upsampling of predictions from \textit{conv7}(convolutionalized \textit{fc7}). Then we continue in this fashion by fusing predictions from \textit{pool3} with a 2 $\times$ upsampling of the 16 stride predictions, denoted as 8 stride predictions. Finally, the 8 stride predictions are upsampled back to image with original size. 
	 
	In our experiments, we first directly apply the FCN-8s to extract building rooftop by replacing the loss function with sigmoid cross-entropy loss. In order to achieving better performance, the network is extended to FCN-4s, FCN-2s. Our experiments show that FCN-4s and FCN-2s get the best performance with overall recall of 70.19 $\%$ in break-even point. According to our results, the first row of Fig. \ref{fig:FCN4s-results} indicate that FCN-4s has less ability to handle  objects with small size. The second row of Fig. \ref{fig:FCN4s-results} shows that it is hard for FCN to discriminate buildings with ground if their color is similar. To overcome these drawbacks, a improved FCN is introduced in next section. 
	 
\begin{figure}
\centering
\label{fig:Fig1FCN8s}
\includegraphics[width=120mm]{FCN8s}
\caption{The framework of FCN-8s}
\end{figure}
	
\begin{figure}
\centering
\subfigure[]{	
	\label{fig:InputImage}
	\includegraphics[width=35mm]{FCN4s-results-Input(a)}}
\subfigure[]{
	\label{fig:GroundTruth}
	\includegraphics[width=35mm]{FCN4s-results-GT(b)}}
\subfigure[]{
	\label{fig:FCN-4s}
	\includegraphics[width=35mm]{FCN4s-results-Prediction(c)}}
\subfigure[]{	
	\label{fig:InputImage}
	\includegraphics[width=35mm]{FCN4s-results-Input(d)}}
\subfigure[]{
	\label{fig:GroundTruth}
	\includegraphics[width=35mm]{FCN4s-results-GT(e)}}
\subfigure[]{
	\label{fig:FCN-4s}
	\includegraphics[width=35mm]{FCN4s-results-Prediction(f)}}
\caption{(a),(d) Input image. (b),(e) Ground truth. (c),(f) FCN-4s prediction.}
\label{fig:FCN4s-results}
\end{figure}
 
\subsection{Our Architecture} 
    Recently, VGGNet \cite{Simonyan2014Very} has been seen to achieve state-of-the-art performance in the ImageNet challenge, with great depth(16 convolutional layers), great density(stride-1 convolutional kernels), and multiple stages(five 2-stride down-sampling layers). Many semantic segmentation architectures \cite{Long2014Fully} \cite{Noh2015Learning}  \cite{chen14semantic} \cite{Zheng2015Conditional} are based on this net. We therefore adopt the VGGNet architecture but make the following modifications:(a) two full connected layers and fifth pooling layer are cut. Because interpolated prediction map of the output of a 32 stride layer is too fuzzy to utilize. (b) We concat 
    
    
   (1) Last stage of VGGNet is cut, including the 5th pooling layer and all the fully connected layers.  (2) In order to integrating multi-scale predictions, we connect some output layers to the last convolutional layer in each stage, respectively conv1$\_$2, conv2$\_$2, conv3$\_$3, conv4$\_$3, conv5$\_$3. Xie and Tu \cite{Xie2015Holistically} have proved this network archive good performance for edge detection. Edge detection work on a very fine level, and our system need to extract many tiny objects, so, it is reasonable to utilize the similar architecture to extract rooftops.
   
 	\begin{tabular}{c|ccccccccccccccc}
	\hline
	layer & c1$\_$1 & c1$\_$2  & c2$\_$1  & c2$\_$2  & c3$\_$1  & c3$\_$2  & c3$\_$3  & c4$\_$1  & c4$\_$2  & c4$\_$3  & c5$\_$1  & c5$\_$2  & c5$\_$3\\
	\hline
    rf size & 3 & 5   & 10  & 14  & 24  & 32  & 40  & 48  & 76  & 92  & 108  & 164  & 196 \\
	\hline
    \end{tabular}
 	
 	
\begin{figure}
\centering
\includegraphics[width=90mm]{ourArchitecture}
\caption{Our Network Architecture}
\label{fig:ourArchitecture}
\end{figure}


\subsection{Formulation}
   Our goal is to predict probabilistic label image $\mathbf{M}$ from an input aerial image $\mathbf{S}$. Fig \ref{fig:ResultsofDifferentSupervision} shows an example of $\mathbf{S}$ and $\mathbf{M}$. We directly learn a mapping from raw pixels in $\mathbf{S}$ to a true label image  $\mathbf{M}$ by training the whole network. 
   
    Here we formulate our approach for building extraction. We denote our input training data set by $\mathbf{I} = \{(\mathbf{S}_{n},\mathbf{M}_{n}),n = 1,\ldots,\vert \mathbf{S}_n \vert \}$, where sample $\mathbf{S}_{n} = \{s_{j}^{(n)}, j = 1,\ldots,\vert \mathbf{S}_n \vert \}$ denotes the raw input image and  $\mathbf{M}_{n} = \{m_{j}^{(n)}, j = 1,\ldots,\vert \mathbf{S_n} \vert\}$, $m_j^{(n)} \in \{0,1\}$ denotes the corresponding ground truth binary labelling map for satellite image $\mathbf{S}_{n}$.  Taking account of each image holistically and independently, thus, we adopt the subscript $n$ for notational simplicity. Our goal is to have a network that learns features from which it is possible to produce building maps approaching the ground truth. 
    In our image-to-image training, the loss function is computed over all pixels in a training image $\mathbf{S} = \{s_{j}, j = 1,\ldots,\vert \mathbf{S} \vert\}$ and building map $\mathbf{M} = \{m_{j}, j = 1,\ldots,\vert \mathbf{S} \vert\}$, $m_j \in \{0,1\}$.
For simplicity, we denote the collection of all standard network layer parameters as $\mathbf{W}$. For each pixel $j$ in a training image, the possibility that assigns it to building is denoted as $\hat{m}_j = Pr(m_j = 1|\mathbf{S};\mathbf{W})$. Specifically, the definition of sigmoid cross-entropy loss function is shown in Eq (\ref{single-loss}).
\begin{equation}
	\label{single-loss}
    \mathcal{L}_{single} = - \sum_{j \in \mathbf{S}} \left[ m_j \log{\hat{m}_j} + (1 - m_j)\log{(1 - \hat{m}_j)} \right]
\end{equation}

% \begin{figure}
% \label{OutRepresentation}
% \centering
% \subfigure[Aerial image]{	
%	\label{fig:AerialImage}
%	\includegraphics[width=30mm]{OutRepresentation(AerialImage)}}
% \subfigure[Region Mask]{
%	\label{fig:LabellingMap}
%	\includegraphics[width=30mm]{OutRepresentation(LabellingMap)}}
% \subfigure[Signed Distance Map ]{
%	\label{fig:SingleLoss}
%	\includegraphics[width=30mm]{OutRepresentation(SignedDistanceMap)}}
% \caption{Two output representations}
% \end{figure}
%===========================================================
\section{Experiments}
In this section, we discuss our detailed implementation and report the performance of our proposed algorithm.

\subsection{Dataset}
In our experiments, we use \textit{Massachusetts Buildings Dataset} (Mass. Buildings) proposed by Mnih \cite{Mnih2013Machine} and publicly available on website
http://www.cs.toronto.e
du/~vmnih/data/. The dataset consists of 151 aerial images of the Boston area, with each of the images being 1500 $\times$ 1500 pixels for an area of 2.25 square
kilometers. Hence, the entire dataset covers roughly 340 square kilometers. The data is split into a training set of 137 images, a test set of 10 images and
a validation set of 4 images. To train the network, we create image tiles of size 256 $\times$ 256 by means of cropping entire image using a sliding window with size of 256 $\times$ 256 and stride of 64 pixels. When scanning the whole dataset, image tiles with too many white pixels are removed. After scanning, train and validation dataset consists of 75938 and 2500 tiles and corresponding building masks. For testing, we use ten 1500 $\times$ 1500 entire images covering area excluded from the training data. 
In our experiments, input image should be scaled into range [0,1]. 

\subsection{Implementation}
The implementation of our framework is based on the publicly  available \textit{Caffe} \cite{Jia2014Caffe} Library. In our system, the whole network is fine-tuned from an initialization with the pre-trained VGG-16 Net model. Thus, it cost about four hour for training the network after 4000 iterations on a single NVIDIA Titan 12GB GPU.

The network is trained in an end-to-end manner. No pre or post-processing is used. We train the networking using stochastic gradient descent with 20 images as a mini-batch. The weight update rule is used with fixed learning rate $10^{-5}$, momentum 0.9, and weight decay $5\times 10^{-3}$. To prevent gradient decreasing significantly, clip$\_$gradients is set to 10000. 

\subsection{Results}

 \begin{figure}
\centering
\includegraphics[width=120mm]{ComparedResults}
\caption{(a) Input image. (b) Ground Truth. (c) Mnih's result. (d) Saito's result}
\label{fig:BadResults}
\end{figure}

%\begin{figure}
%\centering
%\subfigure[Aerial image $\mathbf{S}$]{	
%	\label{fig:AerialImage}
%	\includegraphics[width=30mm]{Fig4(a)}}
%\subfigure[Labelling map $\mathbf{M}$]{
%	\label{fig:LabellingMap}
%	\includegraphics[width=30mm]{Fig4(b)}}
%\subfigure[Single supervision predicted result $\hat{\mathbf{M}}_{single}$ ]{
%	\label{fig:SingleLoss}
%	\includegraphics[width=30mm]{Fig4(c)}}
%\subfigure[Double supervision predicted result $\hat{\mathbf{M}}_{double}$ ]{
%	\label{fig:DoubleLoss}
%	\includegraphics[width=30mm]{Fig4(d )}}
%\caption{Compared results of using different supervisions}
%\label{ResultsofDifferentSupervision}
%\end{figure}

%===========================================================
\section{Conclusions}


%===========================================================
\bibliographystyle{splncs}
\bibliography{egbib}

%this would normally be the end of your paper, but you may also have an appendix
%within the given limit of number of pages
\end{document}
